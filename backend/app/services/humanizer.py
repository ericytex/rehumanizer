from fastapi import APIRouter, HTTPException, UploadFile, File
from pydantic import BaseModel
import os
import random
import re
import time
import logging
from time import perf_counter
from typing import Optional
# Try to import transformers (optional for enhanced features)
try:
    from transformers import PegasusForConditionalGeneration, PegasusTokenizer
    import torch
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    PegasusForConditionalGeneration = None
    PegasusTokenizer = None
from dotenv import load_dotenv

# Import the advanced Gemini pipeline and WriteHuman mimicry
from .gemini_pipeline import run_advanced_pipeline, run_quick_humanization_pipeline
from .writehuman_mimic import WriteHumanMimic
from .enhanced_writehuman import EnhancedWriteHumanMimic
from .fluency_polisher import SafeFluencyPolisher
from .rule_based_polisher import RuleBasedPolisher

# Load environment variables
load_dotenv()

# Try to import Gemini
try:
    import google.generativeai as genai
    GEMINI_AVAILABLE = True
except ImportError:
    GEMINI_AVAILABLE = False
    print("âš ï¸ Google Generative AI not available. Install with: pip install google-generativeai")

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

router = APIRouter()

# --- Gemini model setup ---
API_KEY = os.getenv("GEMINI_API_KEY", "YOUR_GEMINI_API_KEY")
if GEMINI_AVAILABLE and API_KEY != "YOUR_GEMINI_API_KEY" and API_KEY != "YOUR_ACTUAL_GEMINI_API_KEY_HERE":
    genai.configure(api_key=API_KEY)
    model = genai.GenerativeModel("gemini-1.5-pro")
else:
    model = None
    if not GEMINI_AVAILABLE:
        print("âš ï¸ Gemini API not available - install with: pip install google-generativeai")
    else:
        print("âš ï¸ Gemini API not configured - set GEMINI_API_KEY in .env file")
        print("   Get your API key from: https://makersuite.google.com/app/apikey")

# --- Prompt Templates ---
PROMPTS = {
    "humanize_ai_text": (
        "Transform this AI-generated text into natural, human-sounding content while maintaining the same length and core meaning. "
        "Make it conversational, add personal touches, vary sentence structure, and use casual language. "
        "Keep the core meaning but make it sound like a human wrote it. "
        "IMPORTANT: Maintain the same approximate word count as the original text. "
        "Return only the humanized text.\n\n"
        "AI TEXT:\n{text}\n\nHUMANIZED TEXT:"
    ),
}

# --- Helpers ---
def model_call_with_retry(client, prompt, temperature=0.4, max_tokens=600, retries=3):
    """Call the Gemini model with retry logic."""
    for attempt in range(retries):
        try:
            logger.info(f"Model call attempt {attempt + 1}")
            response = client.generate_content(prompt, generation_config={"temperature": temperature, "max_output_tokens": max_tokens})
            text = response.text.strip()
            return {"text": clean_text(text)}
        except Exception as e:
            logger.warning(f"Attempt {attempt + 1} failed: {e}")
            time.sleep(2 ** attempt)
    raise RuntimeError("Model call failed after retries.")

def clean_text(text: str) -> str:
    """Remove intros, labels, and extra formatting from model output."""
    lines = text.strip().split("\n")
    cleaned_lines = []
    for line in lines:
        line = line.strip()
        if not line:
            continue
        # Remove common unwanted prefixes
        for prefix in ["FINAL:", "FINAL ESSAY:", "DRAFT:", "REVISED ESSAY:", "ESSAY:", "SUMMARY:", "Here is", "Here's", "Output:", "HUMANIZED TEXT:"]:
            if line.startswith(prefix):
                line = line[len(prefix):].strip()
        cleaned_lines.append(line)
    return " ".join(cleaned_lines)

def enforce_min_word_count(text: str, min_words: int = 250) -> str:
    """Ensure text meets the minimum word count by expanding if necessary."""
    words = text.split()
    if len(words) >= min_words:
        return text
    
    print(f"âš ï¸ Text only {len(words)} words; expanding to at least {min_words} words...")
    
    # Extract key concepts from the original text
    key_concepts = extract_key_concepts(text)
    
    # Generate comprehensive content based on the key concepts
    expanded_content = generate_comprehensive_content(key_concepts, min_words, text)
    
    return expanded_content

def extract_key_concepts(text: str) -> list:
    """Extract key concepts from the text for expansion."""
    # Simple concept extraction - in a real system, you might use NLP
    concepts = []
    text_lower = text.lower()
    
    if 'artificial intelligence' in text_lower or 'ai' in text_lower:
        concepts.append('artificial intelligence')
    if 'natural language processing' in text_lower or 'nlp' in text_lower:
        concepts.append('natural language processing')
    if 'machine learning' in text_lower:
        concepts.append('machine learning')
    if 'text generation' in text_lower:
        concepts.append('text generation')
    if 'pattern' in text_lower:
        concepts.append('pattern recognition')
    if 'communication' in text_lower:
        concepts.append('human communication')
    if 'system' in text_lower:
        concepts.append('computational systems')
    
    return concepts if concepts else ['technology', 'innovation', 'development']

def generate_comprehensive_content(concepts: list, min_words: int, original_text: str) -> str:
    """Generate comprehensive content based on key concepts."""
    
    # Define comprehensive content templates for different concepts
    content_templates = {
        'artificial intelligence': [
            "Artificial intelligence represents one of the most transformative technologies of our time.",
            "The field of AI has evolved dramatically over the past few decades.",
            "Modern AI systems demonstrate capabilities that were once considered science fiction.",
            "Researchers continue to push the boundaries of what AI can accomplish.",
            "The integration of AI into various industries is accelerating rapidly.",
            "Organizations worldwide are investing heavily in AI development and deployment.",
            "The potential applications of AI span virtually every sector of the economy.",
            "Ethical considerations play a crucial role in AI development and deployment.",
            "The future of AI promises even more remarkable advancements.",
            "AI systems are becoming increasingly sophisticated and capable."
        ],
        'natural language processing': [
            "Natural language processing enables computers to understand and generate human language.",
            "NLP technologies have revolutionized how we interact with digital systems.",
            "The complexity of human language presents unique challenges for computational systems.",
            "Modern NLP models can process and generate text with remarkable accuracy.",
            "Applications of NLP range from chatbots to advanced translation services.",
            "The development of large language models has transformed the field.",
            "NLP continues to improve through advances in machine learning.",
            "Real-world applications of NLP are becoming increasingly common.",
            "The technology enables more natural human-computer interactions.",
            "NLP represents a significant milestone in computational linguistics."
        ],
        'machine learning': [
            "Machine learning algorithms can identify patterns in vast amounts of data.",
            "The ability to learn from experience distinguishes ML from traditional programming.",
            "Deep learning has opened new possibilities for complex problem-solving.",
            "ML systems improve their performance through continuous training and refinement.",
            "The technology has applications across numerous industries and domains.",
            "Researchers are constantly developing new approaches to machine learning.",
            "The field continues to evolve with new architectures and methodologies.",
            "Organizations are leveraging ML to gain competitive advantages.",
            "The democratization of ML tools has accelerated innovation.",
            "Machine learning represents a fundamental shift in computational approaches."
        ],
        'text generation': [
            "Text generation capabilities have reached unprecedented levels of sophistication.",
            "Modern systems can produce coherent and contextually appropriate content.",
            "The technology enables automated content creation at scale.",
            "Quality text generation requires understanding of context and nuance.",
            "Applications range from creative writing to technical documentation.",
            "The field continues to advance through improved training methodologies.",
            "Text generation systems are becoming increasingly human-like in their output.",
            "The technology has implications for content creation and communication.",
            "Researchers are exploring new approaches to enhance generation quality.",
            "The potential applications span creative, educational, and commercial domains."
        ],
        'pattern recognition': [
            "Pattern recognition enables systems to identify meaningful structures in data.",
            "The ability to recognize patterns is fundamental to intelligent behavior.",
            "Modern algorithms can identify complex patterns across multiple dimensions.",
            "Pattern recognition has applications in numerous fields and industries.",
            "The technology continues to improve through advances in computational power.",
            "Researchers are developing new approaches to pattern recognition.",
            "The field has implications for understanding human cognition.",
            "Pattern recognition enables more sophisticated decision-making systems.",
            "The technology is becoming increasingly important in data-driven environments.",
            "Future developments promise even more sophisticated pattern recognition capabilities."
        ],
        'human communication': [
            "Human communication involves complex patterns of language and behavior.",
            "Understanding human communication requires sophisticated computational models.",
            "The nuances of human language present unique challenges for AI systems.",
            "Modern systems can process and generate human-like communication patterns.",
            "The technology enables more natural interactions between humans and machines.",
            "Researchers continue to explore the complexities of human communication.",
            "The field has implications for human-computer interaction design.",
            "Understanding communication patterns is crucial for effective AI systems.",
            "The technology continues to evolve toward more natural communication.",
            "Future developments promise even more sophisticated communication capabilities."
        ],
        'computational systems': [
            "Computational systems form the foundation of modern technological infrastructure.",
            "The complexity of these systems continues to increase with technological advancement.",
            "Modern computational systems can process vast amounts of information rapidly.",
            "The integration of various technologies creates powerful synergistic effects.",
            "Computational systems enable new possibilities for problem-solving and innovation.",
            "The field continues to evolve with new architectures and methodologies.",
            "Organizations rely on computational systems for critical operations.",
            "The technology has transformed how we approach complex challenges.",
            "Future developments promise even more sophisticated computational capabilities.",
            "Computational systems represent the backbone of modern technological advancement."
        ],
        'technology': [
            "Technology continues to advance at an unprecedented pace across all domains.",
            "Innovation in technology creates new possibilities for human achievement.",
            "Modern technological systems demonstrate remarkable capabilities and potential.",
            "The integration of various technologies creates powerful synergistic effects.",
            "Technology has transformed virtually every aspect of human society.",
            "Researchers and developers continue to push the boundaries of what's possible.",
            "The potential applications of technology are virtually limitless.",
            "Organizations worldwide are investing heavily in technological development.",
            "The future of technology promises even more remarkable advancements.",
            "Technology represents one of the most powerful forces for human progress."
        ],
        'innovation': [
            "Innovation drives progress across all fields of human endeavor.",
            "The pace of innovation continues to accelerate in the modern era.",
            "Innovative approaches to problem-solving create new possibilities.",
            "The integration of different technologies often leads to breakthrough innovations.",
            "Innovation requires both creativity and systematic methodology.",
            "Researchers and developers constantly explore new approaches and methodologies.",
            "The potential for innovation spans virtually every domain of human activity.",
            "Organizations that embrace innovation gain significant competitive advantages.",
            "The future promises even more remarkable innovations across all fields.",
            "Innovation represents one of the most powerful forces for human advancement."
        ],
        'development': [
            "Development processes continue to evolve with technological advancement.",
            "Modern development methodologies enable more efficient and effective outcomes.",
            "The complexity of development projects continues to increase over time.",
            "Development requires careful consideration of various factors and constraints.",
            "The field continues to advance through improved methodologies and tools.",
            "Researchers and practitioners constantly refine development approaches.",
            "The potential applications of development span numerous domains.",
            "Organizations worldwide invest heavily in development capabilities.",
            "The future of development promises even more sophisticated approaches.",
            "Development represents a crucial component of technological advancement."
        ]
    }
    
    # Generate content based on the concepts
    generated_content = []
    
    # Start with the original text
    generated_content.append(original_text)
    
    # Add content for each concept
    for concept in concepts:
        if concept in content_templates:
            # Add 3-4 sentences for each concept
            concept_content = content_templates[concept][:4]
            generated_content.extend(concept_content)
    
    # Add some general technology content to ensure we reach the minimum
    general_content = content_templates['technology'][:3]
    generated_content.extend(general_content)
    
    # Combine all content
    final_text = '. '.join(generated_content)
    
    # Ensure we have enough words
    final_words = final_text.split()
    if len(final_words) < min_words:
        # Add more content from any available template
        additional_content = []
        for template in content_templates.values():
            additional_content.extend(template)
        
        # Add content until we reach the minimum
        content_index = 0
        while len(final_words) < min_words and content_index < len(additional_content):
            final_text += '. ' + additional_content[content_index]
            final_words = final_text.split()
            content_index += 1
    
    return final_text

class GeminiHumanizer:
    """Advanced humanizer using Gemini API for sophisticated text transformation."""
    
    def __init__(self):
        self.model = model
        self.available = GEMINI_AVAILABLE and model is not None
        if self.available:
            print("âœ… Gemini API is available and configured")
        else:
            print("âš ï¸ Gemini API not available - check GEMINI_API_KEY in .env file")
        
    def humanize_text(self, text: str) -> str:
        """Use Gemini to humanize AI-generated text."""
        if not self.available:
            print("âš ï¸ Skipping Gemini humanization - API not available")
            return text
            
        try:
            print(f"ðŸ¤– Calling Gemini API to humanize text ({len(text.split())} words)...")
            prompt = PROMPTS["humanize_ai_text"].format(text=text)
            result = model_call_with_retry(self.model, prompt, temperature=0.7, max_tokens=800)
            humanized_text = result["text"]
            print(f"âœ… Gemini humanization complete: {len(humanized_text.split())} words")
            return humanized_text
        except Exception as e:
            print(f"âŒ Gemini humanization failed: {e}")
            logger.error(f"Gemini humanization failed: {e}")
            return text

class HumaneyesParaphraser:
    def __init__(self):
        self.available = False
        if TRANSFORMERS_AVAILABLE:
            try:
                model_name = "Eemansleepdeprived/Humaneyes"
                self.tokenizer = PegasusTokenizer.from_pretrained(model_name)
                self.model = PegasusForConditionalGeneration.from_pretrained(model_name)
                self.available = True
                print("âœ… Humaneyes Pegasus model loaded successfully")
            except Exception as e:
                print(f"âš ï¸ Humaneyes model unavailable: {e}")
        else:
            print("âš ï¸ Transformers not available, using simplified paraphrasing")

    def paraphrase(self, text: str) -> str:
        if not self.available:
            # Simple fallback paraphrasing using basic text manipulation
            return self._simple_paraphrase(text)
        
        inputs = self.tokenizer(text, truncation=True, padding='longest', return_tensors="pt")
        summary_ids = self.model.generate(
            **inputs,
            max_length=256,
            num_beams=5,
            num_return_sequences=1,
            temperature=1.0
        )
        paraphrased = self.tokenizer.decode(summary_ids[0], skip_special_tokens=True)
        return paraphrased
    
    def _simple_paraphrase(self, text):
        """Fallback paraphrasing when Humaneyes is unavailable"""
        # Basic synonym replacement and sentence restructuring
        simple_replacements = {
            "artificial intelligence": "AI technology",
            "demonstrates": "shows",
            "capabilities": "abilities", 
            "processing": "handling",
            "systems": "platforms",
            "technologies": "tools",
            "algorithms": "methods",
            "sophisticated": "advanced",
            "remarkable": "great",
            "innovative": "new"
        }
        
        result = text
        for original, replacement in simple_replacements.items():
            result = result.replace(original, replacement)
        
        return result

class StylometricHumanizer:
    FILLER_PHRASES = [
        "to be honest", 
        "you know", 
        "as far as I can tell", 
        "if I'm not mistaken",
        "come to think of it"
    ]
    CLAUSE_STARTERS = [
        "Interestingly", 
        "In a way", 
        "Now that I think about it", 
        "Oddly enough"
    ]
    LEXICAL_VARIANTS = {
        "important": ["crucial", "vital", "essential", "significant"],
        "good": ["solid", "decent", "admirable", "commendable"],
        "bad": ["flawed", "lacking", "problematic", "subpar"],
        "use": ["utilize", "employ", "apply", "make use of"],
        "shows": ["demonstrates", "reveals", "indicates", "highlights"]
    }
    FUNCTION_WORDS = ["the", "of", "in", "to", "for", "with", "on", "at"]

    def validate_input(self, text: str):
        if not text or len(text.strip()) < 10:
            return False, "Text must be at least 10 characters long"
        return True, None

    def humanize_text(self, text: str):
        start_time = perf_counter()
        
        original = text
        # Apply stylometric disruptions step-by-step
        text = self.lexical_diversify(text)
        text = self.adjust_function_words(text)
        text = self.sentence_restructure(text)
        text = self.insert_human_variance(text)
        text = self.adjust_sentence_lengths(text)
        text = self.punctuate_variably(text)
        
        # Fake AI detection scoring (replace with real detector API if available)
        score_before = self.fake_ai_detector_score(original)
        score_after = max(0.0, score_before - random.uniform(0.3, 0.45))
        
        end_time = perf_counter()
        
        return {
            "original_text": original,
            "humanized_text": text,
            "processing_time_ms": int((end_time - start_time) * 1000),
            "ai_detection_score_before": round(score_before, 2),
            "ai_detection_score_after": round(score_after, 2),
            "readability_improvement": round(random.uniform(0.1, 0.3), 2)
        }

    def lexical_diversify(self, text: str):
        for word, variants in self.LEXICAL_VARIANTS.items():
            if re.search(rf"\b{word}\b", text, flags=re.IGNORECASE):
                text = re.sub(
                    rf"\b{word}\b",
                    lambda _: random.choice(variants),
                    text,
                    flags=re.IGNORECASE
                )
        return text

    def adjust_function_words(self, text: str):
        words = text.split()
        for i in range(len(words)):
            if words[i].lower() in self.FUNCTION_WORDS and random.random() < 0.15:
                words[i] = words[i] + " really"
        return " ".join(words)

    def sentence_restructure(self, text: str):
        sentences = re.split(r'(?<=[.!?]) +', text)
        if len(sentences) > 1:
            random.shuffle(sentences)
        return " ".join(sentences)

    def insert_human_variance(self, text: str):
        sentences = re.split(r'(?<=[.!?]) +', text)
        for i in range(len(sentences)):
            if random.random() < 0.25:
                sentences[i] = f"{random.choice(self.CLAUSE_STARTERS)}, {sentences[i].lower()}"
            if random.random() < 0.2:
                sentences[i] += f", {random.choice(self.FILLER_PHRASES)}."
        return " ".join(sentences)

    def adjust_sentence_lengths(self, text: str):
        sentences = re.split(r'(?<=[.!?]) +', text)
        new_sentences = []
        for s in sentences:
            if len(s.split()) > 18 and random.random() < 0.5:
                words = s.split()
                cut = random.randint(8, len(words) - 5)
                new_sentences.append(" ".join(words[:cut]) + ".")
                new_sentences.append(" ".join(words[cut:]))
            elif len(s.split()) < 8 and random.random() < 0.3:
                if new_sentences:
                    new_sentences[-1] = new_sentences[-1].rstrip(".") + ", " + s.lower()
                else:
                    new_sentences.append(s)
            else:
                new_sentences.append(s)
        return " ".join(new_sentences)

    def punctuate_variably(self, text: str):
        punctuation_options = [" â€” ", "â€¦ ", "; "]
        def repl(match):
            if random.random() < 0.35:
                return random.choice(punctuation_options)
            else:
                return match.group(0)
        return re.sub(r"\. ", repl, text)

    def fake_ai_detector_score(self, text: str):
        return random.uniform(0.7, 0.95)

class MultiDetectorObfuscator:
    """Specific obfuscation techniques for different AI detectors."""
    
    def __init__(self):
        self.detector_strategies = {
            'gptzero': self.gptzero_evasion,
            'surferseo': self.surferseo_evasion,
            'undetectable': self.undetectable_evasion,
            'general': self.general_evasion
        }
    
    def gptzero_evasion(self, text: str) -> str:
        """GPTZero focuses on perplexity and burstiness."""
        # GPTZero looks for consistent perplexity patterns
        sentences = re.split(r'(?<=[.!?]) +', text)
        modified = []
        
        for i, sentence in enumerate(sentences):
            if i % 3 == 0:  # Every 3rd sentence - make it very simple
                words = sentence.split()[:8]  # Truncate to 8 words
                modified.append(" ".join(words) + ".")
            elif i % 3 == 1:  # Next sentence - make it complex
                # Add subordinate clauses
                words = sentence.split()
                if len(words) > 5:
                    insert_pos = len(words) // 2
                    words.insert(insert_pos, "which, considering the broader context,")
                modified.append(" ".join(words))
            else:  # Third sentence - medium complexity
                modified.append(sentence)
        
        return " ".join(modified)
    
    def surferseo_evasion(self, text: str) -> str:
        """SurferSEO focuses on semantic patterns and coherence."""
        # SurferSEO detects overly coherent structure
        sentences = re.split(r'(?<=[.!?]) +', text)
        
        # Insert random tangents
        tangents = [
            "Actually, let me step back for a second.",
            "Wait, that reminds me of something.",
            "But here's the thing that really gets me.",
            "Okay, so I'm getting ahead of myself here.",
            "You know what though?",
        ]
        
        modified = []
        for i, sentence in enumerate(sentences):
            modified.append(sentence)
            # 15% chance to add a tangent
            if random.random() < 0.15:
                modified.append(random.choice(tangents))
        
        return " ".join(modified)
    
    def undetectable_evasion(self, text: str) -> str:
        """Undetectable.ai focuses on vocabulary sophistication."""
        # Replace sophisticated words with simpler alternatives
        sophisticated_replacements = {
            'utilize': 'use',
            'demonstrate': 'show',
            'facilitate': 'help',
            'consequently': 'so',
            'furthermore': 'also',
            'nevertheless': 'but',
            'substantial': 'big',
            'comprehend': 'understand',
            'implement': 'do',
            'analyze': 'look at'
        }
        
        for sophisticated, simple in sophisticated_replacements.items():
            text = re.sub(rf'\b{sophisticated}\b', simple, text, flags=re.IGNORECASE)
        
        return text
    
    def general_evasion(self, text: str) -> str:
        """General evasion techniques."""
        # Add inconsistent spacing and punctuation
        text = re.sub(r'\.(\w)', r'. \1', text)  # Fix spacing after periods
        text = re.sub(r',(\w)', r', \1', text)   # Fix spacing after commas
        
        # Add occasional double spaces
        if random.random() < 0.1:
            text = text.replace('. ', '.  ', 1)
        
        return text
    
    def apply_multi_detector_evasion(self, text: str) -> str:
        """Apply all detector-specific evasion techniques."""
        print("ðŸ”„ Applying multi-detector specific evasion...")
        
        for detector, strategy in self.detector_strategies.items():
            text = strategy(text)
            print(f"âœ… Applied {detector} evasion")
        
        return text

class EducationalLevelEngine:
    """Adjusts writing style based on educational level."""
    
    def __init__(self):
        self.level_configs = {
            'elementary': {
                'max_sentence_length': 12,
                'syllable_complexity': 1.5,
                'vocabulary_level': 'basic',
                'sentence_starters': ['I think', 'I like', 'This is', 'We can', 'It is'],
                'conjunctions': ['and', 'but', 'so', 'because']
            },
            'middle_school': {
                'max_sentence_length': 18,
                'syllable_complexity': 2.0,
                'vocabulary_level': 'intermediate',
                'sentence_starters': ['Although', 'However', 'For example', 'In fact'],
                'conjunctions': ['however', 'therefore', 'furthermore', 'meanwhile']
            },
            'high_school': {
                'max_sentence_length': 25,
                'syllable_complexity': 2.5,
                'vocabulary_level': 'advanced',
                'sentence_starters': ['Nevertheless', 'Consequently', 'In contrast', 'Moreover'],
                'conjunctions': ['nevertheless', 'consequently', 'furthermore', 'conversely']
            },
            'undergraduate': {
                'max_sentence_length': 30,
                'syllable_complexity': 3.0,
                'vocabulary_level': 'sophisticated',
                'sentence_starters': ['Subsequently', 'Notwithstanding', 'In accordance with'],
                'conjunctions': ['subsequently', 'notwithstanding', 'correspondingly']
            },
            'masters': {
                'max_sentence_length': 35,
                'syllable_complexity': 3.5,
                'vocabulary_level': 'academic',
                'sentence_starters': ['Empirically speaking', 'From a theoretical perspective', 'Methodologically'],
                'conjunctions': ['empirically', 'theoretically', 'methodologically', 'systematically']
            },
            'phd': {
                'max_sentence_length': 40,
                'syllable_complexity': 4.0,
                'vocabulary_level': 'scholarly',
                'sentence_starters': ['Epistemologically', 'From a paradigmatic standpoint', 'Phenomenologically'],
                'conjunctions': ['epistemologically', 'paradigmatically', 'phenomenologically', 'hermeneutically']
            }
        }
        
        self.vocabulary_replacements = {
            'basic': {
                'significant': 'big',
                'demonstrate': 'show',
                'utilize': 'use',
                'comprehend': 'understand'
            },
            'intermediate': {
                'significant': 'important',
                'demonstrate': 'prove',
                'utilize': 'employ',
                'comprehend': 'grasp'
            },
            'advanced': {
                'significant': 'substantial',
                'demonstrate': 'illustrate',
                'utilize': 'leverage',
                'comprehend': 'discern'
            },
            'sophisticated': {
                'significant': 'considerable',
                'demonstrate': 'elucidate',
                'utilize': 'deploy',
                'comprehend': 'apprehend'
            },
            'academic': {
                'significant': 'substantive',
                'demonstrate': 'exemplify',
                'utilize': 'operationalize',
                'comprehend': 'conceptualize'
            },
            'scholarly': {
                'significant': 'ontologically substantial',
                'demonstrate': 'empirically validate',
                'utilize': 'methodologically deploy',
                'comprehend': 'epistemologically apprehend'
            }
        }
    
    def adjust_to_level(self, text: str, level: str = 'undergraduate') -> str:
        """Adjust text to specific educational level."""
        if level not in self.level_configs:
            level = 'undergraduate'
        
        config = self.level_configs[level]
        print(f"ðŸ”„ Adjusting text to {level} level...")
        
        # Adjust vocabulary
        text = self.adjust_vocabulary(text, config['vocabulary_level'])
        
        # Adjust sentence complexity
        text = self.adjust_sentence_complexity(text, config)
        
        # Add level-appropriate transitions
        text = self.add_level_transitions(text, config)
        
        print(f"âœ… Text adjusted to {level} level")
        return text
    
    def adjust_vocabulary(self, text: str, vocab_level: str) -> str:
        """Replace words based on vocabulary level."""
        if vocab_level in self.vocabulary_replacements:
            replacements = self.vocabulary_replacements[vocab_level]
            for original, replacement in replacements.items():
                text = re.sub(rf'\b{original}\b', replacement, text, flags=re.IGNORECASE)
        return text
    
    def adjust_sentence_complexity(self, text: str, config: dict) -> str:
        """Adjust sentence length and complexity."""
        sentences = re.split(r'(?<=[.!?]) +', text)
        modified = []
        
        max_length = config['max_sentence_length']
        
        for sentence in sentences:
            words = sentence.split()
            if len(words) > max_length:
                # Split long sentences
                mid_point = len(words) // 2
                first_half = " ".join(words[:mid_point]) + "."
                second_half = " ".join(words[mid_point:])
                modified.extend([first_half, second_half])
            else:
                modified.append(sentence)
        
        return " ".join(modified)
    
    def add_level_transitions(self, text: str, config: dict) -> str:
        """Add appropriate transition words for the level."""
        sentences = text.split('. ')
        
        # Add transitions to 20% of sentences
        for i in range(len(sentences)):
            if random.random() < 0.20:
                starter = random.choice(config['sentence_starters'])
                sentences[i] = f"{starter}, {sentences[i].lower()}"
        
        return '. '.join(sentences)

class PerplexityOptimizer:
    """Optimizes text perplexity to match human writing patterns."""
    
    def __init__(self):
        self.target_perplexity_ranges = {
            'elementary': (20, 40),
            'middle_school': (30, 50),
            'high_school': (40, 65),
            'undergraduate': (50, 80),
            'masters': (60, 90),
            'phd': (70, 100)
        }
    
    def calculate_simple_perplexity(self, text: str) -> float:
        """Simple perplexity estimation based on word frequency."""
        words = text.lower().split()
        if len(words) < 2:
            return 50.0
        
        # Count word frequencies
        word_freq = {}
        for word in words:
            word_freq[word] = word_freq.get(word, 0) + 1
        
        # Calculate perplexity-like score
        total_words = len(words)
        unique_words = len(word_freq)
        
        # Higher ratio of unique words = higher perplexity
        perplexity = (unique_words / total_words) * 100
        
        return min(perplexity, 100.0)
    
    def optimize_perplexity(self, text: str, level: str = 'undergraduate') -> str:
        """Optimize text perplexity for target range."""
        target_min, target_max = self.target_perplexity_ranges.get(level, (50, 80))
        current_perplexity = self.calculate_simple_perplexity(text)
        
        print(f"ðŸ”„ Current perplexity: {current_perplexity:.1f}, Target: {target_min}-{target_max}")
        
        if current_perplexity < target_min:
            # Increase perplexity by adding variety
            text = self.increase_perplexity(text)
        elif current_perplexity > target_max:
            # Decrease perplexity by adding repetition
            text = self.decrease_perplexity(text)
        
        final_perplexity = self.calculate_simple_perplexity(text)
        print(f"âœ… Optimized perplexity: {final_perplexity:.1f}")
        
        return text
    
    def increase_perplexity(self, text: str) -> str:
        """Increase perplexity by adding word variety."""
        synonyms = {
            'good': ['excellent', 'great', 'superb', 'outstanding'],
            'bad': ['poor', 'terrible', 'awful', 'dreadful'],
            'big': ['large', 'huge', 'enormous', 'massive'],
            'small': ['tiny', 'minute', 'miniature', 'petite']
        }
        
        for word, synonym_list in synonyms.items():
            if word in text.lower():
                replacement = random.choice(synonym_list)
                text = re.sub(rf'\b{word}\b', replacement, text, count=1, flags=re.IGNORECASE)
        
        return text
    
    def decrease_perplexity(self, text: str) -> str:
        """Decrease perplexity by adding repetition."""
        sentences = text.split('. ')
        if len(sentences) > 2:
            # Repeat some phrases
            common_phrases = ['in fact', 'for example', 'in other words']
            phrase = random.choice(common_phrases)
            
            # Add the phrase to multiple sentences
            for i in range(0, min(3, len(sentences))):
                if random.random() < 0.3:
                    sentences[i] = f"{phrase}, {sentences[i].lower()}"
        
        return '. '.join(sentences)

class EnhancedStylometricHumanizer:
    """Combines Humaneyes paraphrasing, Gemini humanization, and stylometric obfuscation."""
    
    def __init__(self):
        self.paraphraser = HumaneyesParaphraser()
        self.gemini_humanizer = GeminiHumanizer()
        self.stylometric_humanizer = StylometricHumanizer()
        self.multi_detector_obfuscator = MultiDetectorObfuscator()
        self.educational_engine = EducationalLevelEngine()
        self.perplexity_optimizer = PerplexityOptimizer()
        print("ðŸš€ Enhanced ComprehensiveHumanizer initialized with ALL advanced algorithms")
        
    def validate_input(self, text: str):
        return self.stylometric_humanizer.validate_input(text)
        
    def humanize_text(self, text: str, pipeline_type="comprehensive", education_level="undergraduate"):
        start_time = perf_counter()
        original = text
        original_word_count = len(text.split())
        print(f"ðŸ“ Starting ULTIMATE humanization pipeline with {original_word_count} words")
        print(f"ðŸŽ¯ Pipeline: {pipeline_type}, Education Level: {education_level}")
        
        # Step 1: Humaneyes Paraphrasing
        print("ðŸ”„ Step 1: Humaneyes paraphrasing...")
        paraphrased = self.paraphraser.paraphrase(text)
        print(f"âœ… Paraphrasing complete: {len(paraphrased.split())} words")
        
        # Step 2: Gemini Humanization (if available)
        gemini_humanized = paraphrased
        if self.gemini_humanizer.available:
            print("ðŸ”„ Step 2: Gemini humanization...")
            try:
                gemini_humanized = self.gemini_humanizer.humanize_text(paraphrased)
                print(f"âœ… Gemini humanization complete: {len(gemini_humanized.split())} words")
            except Exception as e:
                print(f"âš ï¸ Gemini humanization failed: {e}")
                gemini_humanized = paraphrased
        else:
            print("âš ï¸ Gemini not available, skipping...")
        
        # Step 3: Educational Level Adjustment
        print(f"ðŸ”„ Step 3: Adjusting to {education_level} level...")
        level_adjusted = self.educational_engine.adjust_to_level(gemini_humanized, education_level)
        print(f"âœ… Level adjustment complete: {len(level_adjusted.split())} words")
        
        # Step 4: Perplexity Optimization
        print("ðŸ”„ Step 4: Optimizing perplexity...")
        perplexity_optimized = self.perplexity_optimizer.optimize_perplexity(level_adjusted, education_level)
        print(f"âœ… Perplexity optimization complete: {len(perplexity_optimized.split())} words")
        
        # Step 5: Enhanced Stylometric Obfuscation
        print("ðŸ”„ Step 5: Enhanced stylometric obfuscation...")
        result = self.stylometric_humanizer.humanize_text(perplexity_optimized)
        print(f"âœ… Stylometric obfuscation complete: {len(result['humanized_text'].split())} words")
        
        # Step 6: Multi-Detector Specific Evasion
        print("ðŸ”„ Step 6: Multi-detector specific evasion...")
        result["humanized_text"] = self.multi_detector_obfuscator.apply_multi_detector_evasion(result["humanized_text"])
        print(f"âœ… Multi-detector evasion complete: {len(result['humanized_text'].split())} words")
        
        # Step 7: Final Word Count Enforcement
        result["humanized_text"] = enforce_min_word_count(result["humanized_text"], min_words=250)
        final_word_count = len(result["humanized_text"].split())
        print(f"âœ… Final word count: {final_word_count} words")
        
        # Add all intermediate results
        result["paraphrased_text"] = paraphrased
        if self.gemini_humanizer.available:
            result["gemini_humanized_text"] = gemini_humanized
        result["education_level"] = education_level
        
        end_time = perf_counter()
        result["processing_time_ms"] = int((end_time - start_time) * 1000)
        
        print(f"ðŸŽ‰ ULTIMATE pipeline complete! Final: {final_word_count} words (started with {original_word_count})")
        print(f"ðŸ”§ Tools used: Humaneyes âœ…, Gemini {'âœ…' if self.gemini_humanizer.available else 'âŒ'}, Enhanced Stylometric âœ…, Multi-Detector âœ…, Education Level âœ…, Perplexity âœ…")
        
        return result


class MultiDetectorObfuscator:
    """Specific obfuscation techniques for different AI detectors."""
    
    def __init__(self):
        self.detector_strategies = {
            'gptzero': self.gptzero_evasion,
            'surferseo': self.surferseo_evasion,
            'undetectable': self.undetectable_evasion,
            'general': self.general_evasion
        }
    
    def gptzero_evasion(self, text: str) -> str:
        """GPTZero focuses on perplexity and burstiness."""
        # GPTZero looks for consistent perplexity patterns
        sentences = re.split(r'(?<=[.!?]) +', text)
        modified = []
        
        for i, sentence in enumerate(sentences):
            if i % 3 == 0:  # Every 3rd sentence - make it very simple
                words = sentence.split()[:8]  # Truncate to 8 words
                modified.append(" ".join(words) + ".")
            elif i % 3 == 1:  # Next sentence - make it complex
                # Add subordinate clauses
                words = sentence.split()
                if len(words) > 5:
                    insert_pos = len(words) // 2
                    words.insert(insert_pos, "which, considering the broader context,")
                modified.append(" ".join(words))
            else:  # Third sentence - medium complexity
                modified.append(sentence)
        
        return " ".join(modified)
    
    def surferseo_evasion(self, text: str) -> str:
        """SurferSEO focuses on semantic patterns and coherence."""
        # SurferSEO detects overly coherent structure
        sentences = re.split(r'(?<=[.!?]) +', text)
        
        # Insert random tangents
        tangents = [
            "Actually, let me step back for a second.",
            "Wait, that reminds me of something.",
            "But here's the thing that really gets me.",
            "Okay, so I'm getting ahead of myself here.",
            "You know what though?",
        ]
        
        modified = []
        for i, sentence in enumerate(sentences):
            modified.append(sentence)
            # 15% chance to add a tangent
            if random.random() < 0.15:
                modified.append(random.choice(tangents))
        
        return " ".join(modified)
    
    def undetectable_evasion(self, text: str) -> str:
        """Undetectable.ai focuses on vocabulary sophistication."""
        # Replace sophisticated words with simpler alternatives
        sophisticated_replacements = {
            'utilize': 'use',
            'demonstrate': 'show',
            'facilitate': 'help',
            'consequently': 'so',
            'furthermore': 'also',
            'nevertheless': 'but',
            'substantial': 'big',
            'comprehend': 'understand',
            'implement': 'do',
            'analyze': 'look at'
        }
        
        for sophisticated, simple in sophisticated_replacements.items():
            text = re.sub(rf'\b{sophisticated}\b', simple, text, flags=re.IGNORECASE)
        
        return text
    
    def general_evasion(self, text: str) -> str:
        """General evasion techniques."""
        # Add inconsistent spacing and punctuation
        text = re.sub(r'\.(\w)', r'. \1', text)  # Fix spacing after periods
        text = re.sub(r',(\w)', r', \1', text)   # Fix spacing after commas
        
        # Add occasional double spaces
        if random.random() < 0.1:
            text = text.replace('. ', '.  ', 1)
        
        return text
    
    def apply_multi_detector_evasion(self, text: str) -> str:
        """Apply all detector-specific evasion techniques."""
        print("ðŸ”„ Applying multi-detector specific evasion...")
        
        for detector, strategy in self.detector_strategies.items():
            text = strategy(text)
            print(f"âœ… Applied {detector} evasion")
        
        return text


class EducationalLevelEngine:
    """Adjusts writing style based on educational level."""
    
    def __init__(self):
        self.level_configs = {
            'elementary': {
                'max_sentence_length': 12,
                'syllable_complexity': 1.5,
                'vocabulary_level': 'basic',
                'sentence_starters': ['I think', 'I like', 'This is', 'We can', 'It is'],
                'conjunctions': ['and', 'but', 'so', 'because']
            },
            'middle_school': {
                'max_sentence_length': 18,
                'syllable_complexity': 2.0,
                'vocabulary_level': 'intermediate',
                'sentence_starters': ['Although', 'However', 'For example', 'In fact'],
                'conjunctions': ['however', 'therefore', 'furthermore', 'meanwhile']
            },
            'high_school': {
                'max_sentence_length': 25,
                'syllable_complexity': 2.5,
                'vocabulary_level': 'advanced',
                'sentence_starters': ['Nevertheless', 'Consequently', 'In contrast', 'Moreover'],
                'conjunctions': ['nevertheless', 'consequently', 'furthermore', 'conversely']
            },
            'undergraduate': {
                'max_sentence_length': 30,
                'syllable_complexity': 3.0,
                'vocabulary_level': 'sophisticated',
                'sentence_starters': ['Subsequently', 'Notwithstanding', 'In accordance with'],
                'conjunctions': ['subsequently', 'notwithstanding', 'correspondingly']
            },
            'masters': {
                'max_sentence_length': 35,
                'syllable_complexity': 3.5,
                'vocabulary_level': 'academic',
                'sentence_starters': ['Empirically speaking', 'From a theoretical perspective', 'Methodologically'],
                'conjunctions': ['empirically', 'theoretically', 'methodologically', 'systematically']
            },
            'phd': {
                'max_sentence_length': 40,
                'syllable_complexity': 4.0,
                'vocabulary_level': 'scholarly',
                'sentence_starters': ['Epistemologically', 'From a paradigmatic standpoint', 'Phenomenologically'],
                'conjunctions': ['epistemologically', 'paradigmatically', 'phenomenologically', 'hermeneutically']
            }
        }
        
        self.vocabulary_replacements = {
            'basic': {
                'significant': 'big',
                'demonstrate': 'show',
                'utilize': 'use',
                'comprehend': 'understand'
            },
            'intermediate': {
                'significant': 'important',
                'demonstrate': 'prove',
                'utilize': 'employ',
                'comprehend': 'grasp'
            },
            'advanced': {
                'significant': 'substantial',
                'demonstrate': 'illustrate',
                'utilize': 'leverage',
                'comprehend': 'discern'
            },
            'sophisticated': {
                'significant': 'considerable',
                'demonstrate': 'elucidate',
                'utilize': 'deploy',
                'comprehend': 'apprehend'
            },
            'academic': {
                'significant': 'substantive',
                'demonstrate': 'exemplify',
                'utilize': 'operationalize',
                'comprehend': 'conceptualize'
            },
            'scholarly': {
                'significant': 'ontologically substantial',
                'demonstrate': 'empirically validate',
                'utilize': 'methodologically deploy',
                'comprehend': 'epistemologically apprehend'
            }
        }
    
    def adjust_to_level(self, text: str, level: str = 'undergraduate') -> str:
        """Adjust text to specific educational level."""
        if level not in self.level_configs:
            level = 'undergraduate'
        
        config = self.level_configs[level]
        print(f"ðŸ”„ Adjusting text to {level} level...")
        
        # Adjust vocabulary
        text = self.adjust_vocabulary(text, config['vocabulary_level'])
        
        # Adjust sentence complexity
        text = self.adjust_sentence_complexity(text, config)
        
        # Add level-appropriate transitions
        text = self.add_level_transitions(text, config)
        
        print(f"âœ… Text adjusted to {level} level")
        return text
    
    def adjust_vocabulary(self, text: str, vocab_level: str) -> str:
        """Replace words based on vocabulary level."""
        if vocab_level in self.vocabulary_replacements:
            replacements = self.vocabulary_replacements[vocab_level]
            for original, replacement in replacements.items():
                text = re.sub(rf'\b{original}\b', replacement, text, flags=re.IGNORECASE)
        return text
    
    def adjust_sentence_complexity(self, text: str, config: dict) -> str:
        """Adjust sentence length and complexity."""
        sentences = re.split(r'(?<=[.!?]) +', text)
        modified = []
        
        max_length = config['max_sentence_length']
        
        for sentence in sentences:
            words = sentence.split()
            if len(words) > max_length:
                # Split long sentences
                mid_point = len(words) // 2
                first_half = " ".join(words[:mid_point]) + "."
                second_half = " ".join(words[mid_point:])
                modified.extend([first_half, second_half])
            else:
                modified.append(sentence)
        
        return " ".join(modified)
    
    def add_level_transitions(self, text: str, config: dict) -> str:
        """Add appropriate transition words for the level."""
        sentences = text.split('. ')
        
        # Add transitions to 20% of sentences
        for i in range(len(sentences)):
            if random.random() < 0.20:
                starter = random.choice(config['sentence_starters'])
                sentences[i] = f"{starter}, {sentences[i].lower()}"
        
        return '. '.join(sentences)


class PerplexityOptimizer:
    """Optimizes text perplexity to match human writing patterns."""
    
    def __init__(self):
        self.target_perplexity_ranges = {
            'elementary': (20, 40),
            'middle_school': (30, 50),
            'high_school': (40, 65),
            'undergraduate': (50, 80),
            'masters': (60, 90),
            'phd': (70, 100)
        }
    
    def calculate_simple_perplexity(self, text: str) -> float:
        """Simple perplexity estimation based on word frequency."""
        words = text.lower().split()
        if len(words) < 2:
            return 50.0
        
        # Count word frequencies
        word_freq = {}
        for word in words:
            word_freq[word] = word_freq.get(word, 0) + 1
        
        # Calculate perplexity-like score
        total_words = len(words)
        unique_words = len(word_freq)
        
        # Higher ratio of unique words = higher perplexity
        perplexity = (unique_words / total_words) * 100
        
        return min(perplexity, 100.0)
    
    def optimize_perplexity(self, text: str, level: str = 'undergraduate') -> str:
        """Optimize text perplexity for target range."""
        target_min, target_max = self.target_perplexity_ranges.get(level, (50, 80))
        current_perplexity = self.calculate_simple_perplexity(text)
        
        print(f"ðŸ”„ Current perplexity: {current_perplexity:.1f}, Target: {target_min}-{target_max}")
        
        if current_perplexity < target_min:
            # Increase perplexity by adding variety
            text = self.increase_perplexity(text)
        elif current_perplexity > target_max:
            # Decrease perplexity by adding repetition
            text = self.decrease_perplexity(text)
        
        final_perplexity = self.calculate_simple_perplexity(text)
        print(f"âœ… Optimized perplexity: {final_perplexity:.1f}")
        
        return text
    
    def increase_perplexity(self, text: str) -> str:
        """Increase perplexity by adding word variety."""
        synonyms = {
            'good': ['excellent', 'great', 'superb', 'outstanding'],
            'bad': ['poor', 'terrible', 'awful', 'dreadful'],
            'big': ['large', 'huge', 'enormous', 'massive'],
            'small': ['tiny', 'minute', 'miniature', 'petite']
        }
        
        for word, synonym_list in synonyms.items():
            if word in text.lower():
                replacement = random.choice(synonym_list)
                text = re.sub(rf'\b{word}\b', replacement, text, count=1, flags=re.IGNORECASE)
        
        return text
    
    def decrease_perplexity(self, text: str) -> str:
        """Decrease perplexity by adding repetition."""
        sentences = text.split('. ')
        if len(sentences) > 2:
            # Repeat some phrases
            common_phrases = ['in fact', 'for example', 'in other words']
            phrase = random.choice(common_phrases)
            
            # Add the phrase to multiple sentences
            for i in range(0, min(3, len(sentences))):
                if random.random() < 0.3:
                    sentences[i] = f"{phrase}, {sentences[i].lower()}"
        
        return '. '.join(sentences)


class CoherenceDisruptor:
    """
    Step 4: Adds human-like burstiness, mild tangents, punctuation variety,
    and subtle redundancy while preserving meaning and readability.
    Specifically targets GPTZero and SurferSEO detection patterns.
    """
    
    def __init__(self):
        # Tangent phrases (light, relevant, non-breaking)
        self.tangents = [
            "â€”and this is just something I rememberedâ€”",
            "(oddly enough, this ties back to a point I read elsewhere)",
            "â€¦though that's just my personal take on it",
            "â€”which, in an unexpected way, reminds me of something elseâ€”",
            "(funny how these things connect when you think about it)",
            "â€”and I can't help but think about thisâ€”",
            "(this actually reminds me of a broader pattern)",
            "â€¦which is interesting when you consider it",
            "â€”though that might just be my perspectiveâ€”",
            "(and here's where it gets really fascinating)"
        ]
        
        # Human-like emphasis patterns
        self.emphasis_starters = [
            "In other words, ",
            "Put differently, ",
            "To put it another way, ",
            "What I mean is, ",
            "Essentially, ",
            "In essence, ",
            "The key point being, ",
            "More simply put, "
        ]
    
    def disrupt_coherence(self, text: str, paranoid_mode: bool = True) -> str:
        """
        Adds human-like burstiness and statistical irregularities to confuse
        GPTZero and SurferSEO while preserving meaning and readability.
        """
        print("ðŸ”„ Applying coherence disruption for GPTZero & SurferSEO evasion...")
        
        # Split into sentences
        sentences = re.split(r'(?<=[.!?])\s+', text.strip())
        humanized_sentences = []
        
        for i, sentence in enumerate(sentences):
            if not sentence.strip():
                continue
            
            # Step 1: Randomly insert tangents in some sentences
            if random.random() < (0.20 if paranoid_mode else 0.10):  # Higher chance in paranoid mode
                insert_pos = random.randint(1, max(1, len(sentence.split()) - 1))
                words = sentence.split()
                tangent = random.choice(self.tangents)
                words.insert(insert_pos, tangent)
                sentence = " ".join(words)
                print(f"  âœ… Added tangent: {tangent[:30]}...")
            
            # Step 2: Add sentence length variation
            if random.random() < 0.20 and humanized_sentences:  # Merge with previous (create long sentences)
                prev = humanized_sentences.pop()
                sentence = prev.rstrip(".!?") + ", " + sentence.lower()
                print("  âœ… Merged sentences for length variation")
            
            if random.random() < 0.12:  # Make it very short (burstiness)
                words = sentence.split(",")
                if len(words) > 1:
                    sentence = words[0] + "."
                    print("  âœ… Created short sentence for burstiness")
            
            # Step 3: Add punctuation variation
            if random.random() < 0.18:
                sentence = sentence.replace(",", " â€”", 1)
                print("  âœ… Added em-dash variation")
            
            if random.random() < 0.08:
                sentence = sentence.replace(" and ", " & ", 1)
                print("  âœ… Added ampersand variation")
            
            # Step 4: Mild redundancy for emphasis (human thought loops)
            if random.random() < (0.15 if paranoid_mode else 0.08):
                emphasis = random.choice(self.emphasis_starters)
                # Create a simplified version of the sentence for emphasis
                simple_version = self._simplify_for_emphasis(sentence)
                sentence += f" {emphasis}{simple_version.lower()}"
                print(f"  âœ… Added emphasis redundancy: {emphasis}")
            
            # Step 5: Add ellipses for human-like pauses
            if random.random() < 0.10:
                # Insert ellipses at natural pause points
                words = sentence.split()
                if len(words) > 6:
                    pause_pos = random.randint(3, len(words) - 3)
                    words.insert(pause_pos, "...")
                    sentence = " ".join(words)
                    print("  âœ… Added ellipses pause")
            
            humanized_sentences.append(sentence)
        
        # Step 6: Join with strategic paragraph breaks (human writing patterns)
        final_text = ""
        for i, sentence in enumerate(humanized_sentences):
            final_text += sentence + " "
            # Add paragraph breaks at irregular intervals (human-like)
            if i > 0 and i % random.randint(3, 7) == 0:
                final_text += "\n\n"
                print("  âœ… Added paragraph break")
        
        result = final_text.strip()
        print(f"âœ… Coherence disruption complete: {len(result.split())} words")
        print("ðŸŽ¯ Targeting GPTZero & SurferSEO statistical patterns")
        
        return result
    
    def _simplify_for_emphasis(self, sentence: str) -> str:
        """Create a simplified version of a sentence for redundant emphasis."""
        # Remove complex clauses and keep the core message
        simplified = sentence.split(",")[0]  # Take first clause
        simplified = re.sub(r'\([^)]*\)', '', simplified)  # Remove parentheticals
        simplified = re.sub(r'â€”[^â€”]*â€”', '', simplified)  # Remove em-dash clauses
        simplified = simplified.strip()
        
        # If it's too short, return the original
        if len(simplified.split()) < 4:
            return sentence
        
        return simplified


class EnhancedComprehensiveHumanizer:
    """Ultimate humanizer with all advanced algorithms including coherence disruption."""
    
    def __init__(self):
        self.paraphraser = HumaneyesParaphraser()
        self.gemini_humanizer = GeminiHumanizer()
        self.stylometric_humanizer = EnhancedStylometricHumanizer()
        self.multi_detector_obfuscator = MultiDetectorObfuscator()
        self.educational_engine = EducationalLevelEngine()
        self.perplexity_optimizer = PerplexityOptimizer()
        self.coherence_disruptor = CoherenceDisruptor()
        self.writehuman_mimic = WriteHumanMimic(aggressiveness=0.4, seed=42)
        self.enhanced_writehuman = EnhancedWriteHumanMimic(aggressiveness=0.25, semantic_threshold=0.85, seed=42)
        self.fluency_polisher = SafeFluencyPolisher()
        self.rule_based_polisher = RuleBasedPolisher()
        print(f"ðŸš€ Enhanced ComprehensiveHumanizer initialized with ALL advanced algorithms + Coherence Disruption + WriteHuman Mimicry + Semantic Awareness + Fluency Polishing (Transformers: {'âœ…' if TRANSFORMERS_AVAILABLE else 'âš ï¸'})")
    
    def validate_input(self, text: str):
        return self.stylometric_humanizer.validate_input(text)
    
    def humanize_text(self, text: str, pipeline_type="comprehensive", education_level="undergraduate", paranoid_mode=True, writehuman_mode=True):
        start_time = perf_counter()
        original = text
        original_word_count = len(text.split())
        print(f"ðŸ“ Starting ULTIMATE humanization pipeline with {original_word_count} words")
        print(f"ðŸŽ¯ Pipeline: {pipeline_type}, Education Level: {education_level}, Paranoid Mode: {paranoid_mode}, WriteHuman Mode: {writehuman_mode}")
        
        # Step 1: Humaneyes Paraphrasing
        print("ðŸ”„ Step 1: Humaneyes paraphrasing...")
        paraphrased = self.paraphraser.paraphrase(text)
        print(f"âœ… Paraphrasing complete: {len(paraphrased.split())} words")
        
        # Step 2: Gemini Humanization (if available)
        gemini_humanized = paraphrased
        if self.gemini_humanizer.available:
            print("ðŸ”„ Step 2: Gemini humanization...")
            try:
                gemini_humanized = self.gemini_humanizer.humanize_text(paraphrased)
                print(f"âœ… Gemini humanization complete: {len(gemini_humanized.split())} words")
            except Exception as e:
                print(f"âš ï¸ Gemini humanization failed: {e}")
                gemini_humanized = paraphrased
        else:
            print("âš ï¸ Gemini not available, skipping...")
        
        # Step 3: Educational Level Adjustment
        print(f"ðŸ”„ Step 3: Adjusting to {education_level} level...")
        level_adjusted = self.educational_engine.adjust_to_level(gemini_humanized, education_level)
        print(f"âœ… Level adjustment complete: {len(level_adjusted.split())} words")
        
        # Step 4: Perplexity Optimization
        print("ðŸ”„ Step 4: Optimizing perplexity...")
        perplexity_optimized = self.perplexity_optimizer.optimize_perplexity(level_adjusted, education_level)
        print(f"âœ… Perplexity optimization complete: {len(perplexity_optimized.split())} words")
        
        # Step 5: Enhanced Stylometric Obfuscation
        print("ðŸ”„ Step 5: Enhanced stylometric obfuscation...")
        result = self.stylometric_humanizer.humanize_text(perplexity_optimized)
        print(f"âœ… Stylometric obfuscation complete: {len(result['humanized_text'].split())} words")
        
        # Step 6: Multi-Detector Specific Evasion
        print("ðŸ”„ Step 6: Multi-detector specific evasion...")
        result["humanized_text"] = self.multi_detector_obfuscator.apply_multi_detector_evasion(result["humanized_text"])
        print(f"âœ… Multi-detector evasion complete: {len(result['humanized_text'].split())} words")
        
        # Step 7: Coherence Disruption (GPTZero & SurferSEO Killer)
        print("ðŸ”„ Step 7: Coherence disruption (targeting GPTZero & SurferSEO)...")
        result["humanized_text"] = self.coherence_disruptor.disrupt_coherence(result["humanized_text"], paranoid_mode)
        print(f"âœ… Coherence disruption complete: {len(result['humanized_text'].split())} words")
        
        # Step 8: Final Word Count Enforcement
        result["humanized_text"] = enforce_min_word_count(result["humanized_text"], min_words=250)
        final_word_count = len(result["humanized_text"].split())
        print(f"âœ… Final word count: {final_word_count} words")
        
        # Step 9: Enhanced WriteHuman Mimicry with Semantic Awareness - Optional
        if writehuman_mode:
            print("ðŸŽ­ Step 9: Enhanced WriteHuman mimicry with semantic awareness...")
            writehuman_result = self.enhanced_writehuman.process(result["humanized_text"], min_words=200)
            result["humanized_text"] = writehuman_result["text"]
            final_final_word_count = len(result["humanized_text"].split())
            print(f"âœ… Enhanced WriteHuman complete: {final_final_word_count} words (similarity: {writehuman_result['similarity']:.3f})")
        else:
            print("â­ï¸ Step 9: Enhanced WriteHuman mimicry skipped (disabled)")
            final_final_word_count = final_word_count
        
        # Step 10: Safe Fluency Polishing with AI Pattern Protection - Optional  
        print("ðŸ”§ Step 10: Safe fluency polishing with AI pattern protection...")
        polish_result = self.fluency_polisher.polish(result["humanized_text"], method="rule_based")
        result["humanized_text"] = polish_result["text"]
        final_polished_word_count = len(result["humanized_text"].split())
        print(f"âœ… Safe polishing complete ({polish_result['method_used']}): {final_polished_word_count} words")
        
        # Add all intermediate results
        result["paraphrased_text"] = paraphrased
        if self.gemini_humanizer.available:
            result["gemini_humanized_text"] = gemini_humanized
        result["education_level"] = education_level
        
        end_time = perf_counter()
        result["processing_time_ms"] = int((end_time - start_time) * 1000)
        
        print(f"ðŸŽ‰ ULTIMATE pipeline complete! Final: {final_polished_word_count} words (started with {original_word_count})")
        print(f"ðŸ”§ Tools used: Humaneyes {'âœ…' if self.paraphraser.available else 'âš ï¸'}, Gemini {'âœ…' if self.gemini_humanizer.available else 'âŒ'}, Enhanced Stylometric âœ…, Multi-Detector âœ…, Education Level âœ…, Perplexity âœ…, Coherence Disruptor âœ…, Enhanced WriteHuman {'âœ…' if writehuman_mode else 'â­ï¸'}, Safe Polish âœ…")
        
        return result


# Initialize the enhanced comprehensive humanizer instance
humanizer = EnhancedComprehensiveHumanizer() 